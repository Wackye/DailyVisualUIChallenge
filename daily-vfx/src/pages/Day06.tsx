import React, { useRef, useEffect, useState, forwardRef, useImperativeHandle } from "react";
// ç›´æ¥å¾ CDN import vision å‡½å¼åº«ï¼Œé€™æ˜¯æˆ‘å€‘æ‰€æœ‰ MediaPipe åŠŸèƒ½çš„ä¾†æº
import {
  FaceLandmarker,
  FilesetResolver,
  DrawingUtils
} from "https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@0.10.3";

// ====================================================================
// SECTION 1: TYPE DEFINITIONS
// ä¿æŒå‹åˆ¥å®šç¾©çš„æ•´æ½”
// ====================================================================

// --- MediaPipe ç›¸é—œå‹åˆ¥ ---
interface NormalizedLandmark { x: number; y: number; z: number; visibility?: number; }
interface FaceLandmarkerResult {
  faceLandmarks: NormalizedLandmark[][];
  faceBlendshapes?: any[]; // æ ¹æ“šéœ€æ±‚å¯ä»¥å®šç¾©æ›´ç²¾ç¢ºçš„å‹åˆ¥
}

// --- å…ƒä»¶ç›¸é—œå‹åˆ¥ ---
type AppStatus = "loading" | "ready" | "error";

interface WebcamDisplayProps {
  status: AppStatus;
  loadingMessage: string;
}

export interface WebcamDisplayHandles {
  videoRef: React.RefObject<HTMLVideoElement | null>;
  canvasRef: React.RefObject<HTMLCanvasElement | null>;
}

// ====================================================================
// SECTION 2: UI COMPONENTS
// é€™äº›æ˜¯ä½¿ç”¨è€…ä»‹é¢çš„æ§‹æˆè¦ç´ 
// ====================================================================

/**
 * ç‹€æ…‹è¦†è“‹å±¤ UI å…ƒä»¶
 * @description ç•¶ App æ­£åœ¨è¼‰å…¥æ™‚ï¼Œé¡¯ç¤ºä¸€å€‹åŠé€æ˜çš„é®ç½©å’Œæç¤ºè¨Šæ¯
 */
const StatusOverlay: React.FC<{ status: AppStatus; message: string }> = ({ status, message }) => {
  if (status !== "loading") return null;
  return (
    <div className="absolute inset-0 z-20 flex items-center justify-center rounded-lg bg-gray-900/80 backdrop-blur-sm">
      <p className="animate-pulse text-lg font-semibold text-white">{message}</p>
    </div>
  );
};

/**
 * æ”å½±æ©Ÿé¡¯ç¤ºå€ UI å…ƒä»¶
 * @description åŒ…å« video å’Œ canvas å…ƒç´ ï¼Œä¸¦ä½¿ç”¨ forwardRef å°‡å®ƒå€‘çš„ ref å‚³éçµ¦çˆ¶å…ƒä»¶
 */
const WebcamDisplay = forwardRef<WebcamDisplayHandles, WebcamDisplayProps>(({ status, loadingMessage }, ref) => {
  const videoRef = useRef<HTMLVideoElement>(null);
  const canvasRef = useRef<HTMLCanvasElement>(null);
  
  // é€é useImperativeHandle å°‡å…§éƒ¨ ref æš´éœ²çµ¦çˆ¶å…ƒä»¶
  useImperativeHandle(ref, () => ({ videoRef, canvasRef }));

  return (
    <div className="relative w-full overflow-hidden rounded-lg bg-gray-900 shadow-xl aspect-video">
      <StatusOverlay status={status} message={loadingMessage} />
      {/* video å…ƒç´ è² è²¬æ¥æ”¶æ”å½±æ©Ÿä¸²æµï¼Œä½†æˆ‘å€‘å°‡å®ƒéš±è—ï¼Œå› ç‚ºå¯¦éš›é¡¯ç¤ºçš„æ˜¯ canvas */}
      <video ref={videoRef} className="absolute w-full h-full -scale-x-100" autoPlay playsInline style={{ visibility: 'hidden' }}/>
      {/* canvas å…ƒç´ è² è²¬ç¹ªè£½å½±åƒå’Œè‡‰éƒ¨ç¶²æ ¼ */}
      <canvas ref={canvasRef} className="absolute inset-0 h-full w-full -scale-x-100" />
    </div>
  );
});


// ====================================================================
// SECTION 3: MAIN COMPONENT
// é€™æ˜¯æ‡‰ç”¨çš„ä¸»é«”
// ====================================================================
const Day06 = () => {
  // --- Refs & State ---
  const displayRef = useRef<WebcamDisplayHandles>(null);
  const faceLandmarkerRef = useRef<FaceLandmarker | null>(null);
  const animationFrameIdRef = useRef<number | null>(null);
  const [status, setStatus] = useState<AppStatus>("loading");
  const [loadingMessage, setLoadingMessage] = useState("æ­£åœ¨æº–å‚™ AI æ¨¡å‹...");

  // --- Effect 1: åˆå§‹åŒ–æ¨¡å‹ä¸¦å•Ÿå‹•æ”å½±æ©Ÿ ---
  useEffect(() => {
    // é€™å€‹ effect åªåœ¨å…ƒä»¶æ›è¼‰æ™‚åŸ·è¡Œä¸€æ¬¡
    const initialize = async () => {
      try {
        console.log("ğŸš€ [æ­¥é©Ÿ 1] é–‹å§‹åˆå§‹åŒ– MediaPipe...");
        setLoadingMessage("æ­£åœ¨è¼‰å…¥ AI æ¨¡å‹æª”æ¡ˆ...");
        
        // å»ºç«‹ vision ä»»å‹™éœ€è¦çš„æª”æ¡ˆè§£æå™¨
        const vision = await FilesetResolver.forVisionTasks(
          "https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@0.10.3/wasm"
        );
        console.log("  âœ… Vision Wasm æª”æ¡ˆè§£æå™¨å»ºç«‹æˆåŠŸã€‚");

        // å»ºç«‹ FaceLandmarker å¯¦ä¾‹
        faceLandmarkerRef.current = await FaceLandmarker.createFromOptions(vision, {
          baseOptions: {
            modelAssetPath: `https://storage.googleapis.com/mediapipe-models/face_landmarker/face_landmarker/float16/1/face_landmarker.task`,
            delegate: "GPU",
          },
          runningMode: "VIDEO",
          outputFaceBlendshapes: true,
        });
        console.log("  âœ… FaceLandmarker æ¨¡å‹å»ºç«‹æˆåŠŸã€‚");

        setStatus("ready");
        console.log("ğŸ‰ [æ­¥é©Ÿ 1] åˆå§‹åŒ–æˆåŠŸï¼App ç‹€æ…‹æ›´æ–°ç‚º 'ready'ã€‚");

      } catch (e) {
        console.error("âŒ åˆå§‹åŒ– MediaPipe æ¨¡å‹æ™‚ç™¼ç”ŸéŒ¯èª¤:", e);
        setLoadingMessage("æ¨¡å‹è¼‰å…¥å¤±æ•—ï¼Œè«‹æª¢æŸ¥ç¶²è·¯æˆ–ä¸»æ§å°éŒ¯èª¤ã€‚");
        setStatus("error");
      }
    };

    initialize();
  }, []);

  // --- Effect 2: ç•¶æ¨¡å‹æº–å‚™å¥½å¾Œï¼Œå•Ÿå‹•æ”å½±æ©Ÿå’Œåµæ¸¬è¿´åœˆ ---
  useEffect(() => {
    if (status !== "ready" || !displayRef.current?.videoRef.current || !displayRef.current?.canvasRef.current) {
      return;
    }
    
    console.log("ğŸš€ [æ­¥é©Ÿ 2] æ¨¡å‹å·²å°±ç·’ï¼Œæº–å‚™å•Ÿå‹•æ”å½±æ©Ÿã€‚");
    const video = displayRef.current.videoRef.current;
    const canvas = displayRef.current.canvasRef.current;
    const ctx = canvas.getContext("2d");
    if (!ctx) {
        console.error("ç„¡æ³•å–å¾— Canvas 2D context");
        return;
    }

    const drawingUtils = new DrawingUtils(ctx);

    const startWebcam = async () => {
      try {
        console.log("  [æ­¥é©Ÿ 2.1] æ­£åœ¨è«‹æ±‚æ”å½±æ©Ÿæ¬Šé™...");
        const stream = await navigator.mediaDevices.getUserMedia({
          video: { width: { ideal: 1280 }, height: { ideal: 720 }, facingMode: 'user' }
        });
        video.srcObject = stream;
        // ç›£è½ 'loadeddata' äº‹ä»¶ï¼Œç¢ºä¿å½±ç‰‡ç¬¬ä¸€å¹€è¼‰å…¥å¾Œæ‰é–‹å§‹åµæ¸¬
        video.addEventListener("loadeddata", startPredictionLoop);
        console.log("  âœ… æ”å½±æ©Ÿæ¬Šé™å·²ç²å–ï¼Œä¸²æµå·²é€£çµã€‚");
      } catch (err) {
        console.error("âŒ ç²å–æ”å½±æ©Ÿæ¬Šé™å¤±æ•—:", err);
        setLoadingMessage("ç„¡æ³•å•Ÿç”¨æ”å½±æ©Ÿï¼Œè«‹æª¢æŸ¥æ¬Šé™ã€‚");
        setStatus("error");
      }
    };

    let lastVideoTime = -1;
    const startPredictionLoop = () => {
        const landmarker = faceLandmarkerRef.current;
        if (!landmarker) return;

        console.log("    [æ­¥é©Ÿ 2.2] å½±ç‰‡æ•¸æ“šå·²è¼‰å…¥ï¼Œé–‹å§‹åµæ¸¬è¿´åœˆã€‚");

        const predict = () => {
            // å¦‚æœå½±ç‰‡ä¸²æµä¸å­˜åœ¨ã€æš«åœæˆ–å·²çµæŸï¼Œå‰‡è·³éæ­¤å¹€
            if (!video.srcObject || video.paused || video.ended) {
                animationFrameIdRef.current = requestAnimationFrame(predict);
                return;
            }

            // åªæœ‰åœ¨å½±ç‰‡æ™‚é–“æˆ³æ›´æ–°æ™‚æ‰é€²è¡Œåµæ¸¬ï¼Œé¿å…é‡è¤‡é‹ç®—
            if (video.currentTime !== lastVideoTime) {
                lastVideoTime = video.currentTime;
                
                // åŸ·è¡Œè‡‰éƒ¨åµæ¸¬
                const results: FaceLandmarkerResult = landmarker.detectForVideo(video, Date.now());

                // åŒæ­¥ canvas å’Œ video çš„å°ºå¯¸
                if (canvas.width !== video.videoWidth) {
                    canvas.width = video.videoWidth;
                    canvas.height = video.videoHeight;
                }

                // æ¸…ç©ºç•«å¸ƒä¸¦ç¹ªè£½ç•¶å‰å½±åƒ
                ctx.clearRect(0, 0, canvas.width, canvas.height);
                ctx.drawImage(video, 0, 0, canvas.width, canvas.height);

                // å¦‚æœåµæ¸¬åˆ°è‡‰éƒ¨ï¼Œå°±ç¹ªè£½ç¶²æ ¼
                if (results.faceLandmarks && results.faceLandmarks.length > 0) {
                  for (const landmarks of results.faceLandmarks) {
                    drawingUtils.drawConnectors(landmarks, FaceLandmarker.FACE_LANDMARKS_TESSELATION, { color: "#C0C0C070", lineWidth: 0.5 });
                    drawingUtils.drawConnectors(landmarks, FaceLandmarker.FACE_LANDMARKS_CONTOURS, { color: "#4dffc9", lineWidth: 1 });
                  }
                }
            }
            animationFrameIdRef.current = requestAnimationFrame(predict);
        };
        predict();
    };

    startWebcam();

    // --- æ¸…ç†å‡½å¼ ---
    return () => {
      console.log("ğŸ§¹ [æ¸…ç†] æ­£åœ¨æ¸…ç†æ”å½±æ©Ÿèˆ‡åµæ¸¬è¿´åœˆè³‡æº...");
      if (animationFrameIdRef.current) {
        cancelAnimationFrame(animationFrameIdRef.current);
      }
      video.removeEventListener("loadeddata", startPredictionLoop);
      if (video.srcObject instanceof MediaStream) {
        video.srcObject.getTracks().forEach(track => track.stop());
        console.log("  -> æ”å½±æ©Ÿä¸²æµå·²åœæ­¢ã€‚");
      }
    };
  }, [status]); // é€™å€‹ effect æœƒåœ¨ status è®Šç‚º 'ready' æ™‚è§¸ç™¼

  // --- Render ---
  return (
    <div className="flex w-full max-w-4xl flex-col items-center justify-center p-4 font-sans text-white">
        <h1 className="text-center text-3xl font-bold tracking-tight">
          AI è‡‰éƒ¨ç‰¹å¾µåµæ¸¬
        </h1>
        <WebcamDisplay 
          ref={displayRef} 
          status={status}
          loadingMessage={loadingMessage}
        />
        {status === "error" && (
            <div className="rounded-md bg-red-900/50 p-4 text-center text-red-300">
                <p>ç™¼ç”ŸéŒ¯èª¤ï¼š{loadingMessage}</p>
                <p className="text-sm mt-1">è«‹å…è¨±æ”å½±æ©Ÿæ¬Šé™ï¼Œä¸¦ç¢ºèªæ‚¨çš„ç€è¦½å™¨æ”¯æ´ WebGL/WASMã€‚</p>
            </div>
        )}
    </div>
  );
};

export default Day06;
